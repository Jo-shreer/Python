## 💡 What is Pandas?
**Pandas** (short for *Python Data Analysis Library*) is an **open-source library** in Python used for **data manipulation, analysis, and cleaning**.  
It provides powerful, easy-to-use **data structures**:
- **Series** → 1D labeled array (like an Excel column)
- **DataFrame** → 2D labeled table (like an Excel sheet or SQL table)
- Pandas is built on top of **NumPy** and integrates smoothly with tools like
  **Matplotlib**, **Seaborn**, **Scikit-learn**, and **SQLAlchemy**.

  ## ⚙️ Why Pandas?
Without pandas, you’d rely on manual loops or nested lists/dicts — slow and hard to manage.  
Pandas makes data handling **simple, fast, and intuitive**.

✅ **Key Benefits:**
- Fast, vectorized operations (no explicit loops)
- SQL/Excel-like syntax
- Handles millions of rows efficiently
- Works with multiple data sources (CSV, Excel, SQL, JSON, Parquet, S3)
- Perfect for data cleaning and analytics

  ## 🧩 Common Use Cases
| **Use Case** | **Example** |
|---------------|-------------|
| 🧹 Data Cleaning | Handle missing values, duplicates, outliers |
| 🔄 Data Transformation | Filter, merge, reshape, pivot |
| 📊 Data Analysis | Grouping, aggregations, statistical summaries |
| 📈 Visualization | Quick charts using Matplotlib/Seaborn |
| 💾 Input/Output | Read/write CSV, Excel, SQL, JSON, Parquet |
| 🧠 ML Feature Engineering | Scaling, encoding, preparing model inputs |

# 🐼 Pandas Cheatsheet — Quick Reference Guide

## ⚙️ Importing & Setup
```python
import pandas as pd
import numpy as np
```
## Reading Data
```python
pd.read_csv("file.csv")
pd.read_excel("file.xlsx", sheet_name="Sheet1")
pd.read_json("file.json")
pd.read_parquet("file.parquet")
pd.read_sql("SELECT * FROM table", connection)
```
## Writing Data
```python
df.to_csv("output.csv", index=False)
df.to_excel("output.xlsx", index=False)
df.to_json("output.json")
df.to_parquet("output.parquet", index=False)
```

## Inspecting Data
```python
df.head()          # First 5 rows
df.tail(3)         # Last 3 rows
df.shape           # (rows, columns)
df.columns         # Column names
df.info()          # Data types + nulls
df.describe()      # Summary stats (numeric cols)
```

## Selecting Data
```python
df['col']                   # Single column (Series)
df[['col1','col2']]         # Multiple columns
df.iloc[0]                  # First row (by index)
df.iloc[0:3]                # First 3 rows (by position)
df.loc[5]                   # Row with index label 5
df.loc[df['age'] > 30]      # Filter by condition
df.query('age > 30 and dept == "IT"')  # SQL-style filter
```

## Adding & Modifying Columns
```python
df['new_col'] = df['a'] + df['b']
df['flag'] = np.where(df['age'] >= 18, 'Adult', 'Minor')
df.rename(columns={'old':'new'}, inplace=True)
df.drop(columns=['unneeded'], inplace=True)

```

## Missing Data
```python
df.isna().sum()          # Count missing values
df.dropna()              # Drop rows with NA
df.dropna(axis=1)        # Drop columns with NA
df.fillna(0)             # Fill with 0
df.fillna({'col': 99})   # Fill specific column
df.ffill()               # Forward fill
df.bfill()               # Backward fill
```

## Filtering & Conditions
```python
df[df['salary'] > 50000]
df[(df['age'] > 30) & (df['dept'] == 'HR')]
df[df['name'].str.contains('Ali')]
df.query('dept == "Sales" or age < 25')
```

## Aggregations & GroupBy
```python
df.groupby('dept')['salary'].mean()
df.groupby('dept').agg(
    emp_count=('id','count'),
    avg_salary=('salary','mean'),
    max_salary=('salary','max')
)

```

## Transform
```python
df['salary_diff'] = df.groupby('dept')['salary'].transform(lambda x: x - x.mean())

```

## Joins & Merge
```python
pd.merge(df1, df2, on='id', how='inner')   # inner join
pd.merge(df1, df2, on='id', how='left')    # left join
pd.concat([df1, df2], axis=0)              # Stack vertically
pd.concat([df1, df2], axis=1)              # Combine columns
```

## Sorting Data
```python
df.sort_values('age')
df.sort_values(['dept','salary'], ascending=[True, False])
df.sort_index()

```

## Duplicates & Sampling 
```python
df.duplicated().sum()
df.drop_duplicates(subset=['id'], keep='last', inplace=True)
df.sample(frac=0.1, random_state=42)
```
## Dates & Time Series 
```python
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)
df['year'] = df['date'].dt.year
df.resample('M')['sales'].sum()        # Monthly aggregation
df['rolling_avg'] = df['sales'].rolling(7).mean()  # 7-day moving avg
```

## Pivoting & Reshaping 
```python
pd.pivot_table(df, values='sales', index='region', columns='product', aggfunc='sum', fill_value=0)
```
## Melt (Unpivot) 
```python
pd.melt(df, id_vars='id', var_name='month', value_name='sales')
```
## Apply / Map / Lambda 
```python
df['col2'] = df['col1'].map({'Y':'Yes','N':'No'})
df['len_name'] = df['name'].apply(len)
df['bonus'] = df.apply(lambda x: x.salary * 0.1 if x.dept=='HR' else x.salary*0.05, axis=1)
```
## Data Type Conversion 
```python
df['age'] = df['age'].astype(int)
df['dept'] = df['dept'].astype('category')
df['date'] = pd.to_datetime(df['date'])
```
## Useful Commands 
```python
df.value_counts('dept')
df['salary'].between(50000, 100000)
df.nlargest(5, 'salary')
df.nsmallest(3, 'age')
df.sample(10)
```
## Quick Visualization 
```python
import matplotlib.pyplot as plt

df['salary'].plot.hist(bins=10)
df.groupby('dept')['salary'].mean().plot.bar()
plt.title('Average Salary by Department')
plt.show()
```

## File Operations with S3 (Requires s3fs) 
```python
df = pd.read_csv('s3://mybucket/data.csv')
df.to_parquet('s3://mybucket/processed/output.parquet')
```
## Performance Tips
Use proper dtypes (category, int32, float32)
Avoid loops — use vectorized operations
Use chunksize for large CSVs
Drop unused columns early
Use df.query() for fast filtering
Use Parquet over CSV for large data

## Mini ETL Example 
```python
import pandas as pd
import numpy as np

# Load
df = pd.read_csv("employees.csv", parse_dates=['hire_date'])

# Clean
df = df[df['age'] >= 18]
df['tenure'] = (pd.Timestamp.today() - df['hire_date']).dt.days // 365

# Transform
df['level'] = np.where(df['age'] < 30, 'Junior',
              np.where(df['age'] < 50, 'Mid', 'Senior'))

# Aggregate
summary = df.groupby('department').agg(
    employees=('id','count'),
    avg_salary=('salary','mean')
).reset_index()

# Save
summary.to_csv("department_summary.csv", index=False)

```

## Vectorized Mappings & Conditional Transformations
## Concept:
Instead of using Python loops (for), vectorization lets Pandas apply operations to entire columns at once 
— it’s faster and cleaner because it uses optimized C code internally (via NumPy).

```python
#Traditional Python Loop (slow)
import pandas as pd

df = pd.DataFrame({
    'price': [100, 200, 300, 400],
    'quantity': [2, 3, 4, 5]
})

# ❌ BAD: Loop through rows manually
total = []
for i in range(len(df)):
    total.append(df.loc[i, 'price'] * df.loc[i, 'quantity'])
df['total'] = total

print(df)

#Vectorized Operation (fast)
# ✅ GOOD: Operate directly on whole columns
df['total'] = df['price'] * df['quantity']
print(df)


```




