ðŸ“˜ AIRFLOW DAG STRUCTURE & default_args

1ï¸âƒ£ DAG STRUCTURE
â€¢ A DAG in Airflow is a **Python object** that defines a workflow.
â€¢ It contains:
  - **dag_id** â†’ unique identifier for the DAG
  - **default_args** â†’ dictionary of default task parameters
  - **schedule_interval** â†’ defines how often the DAG runs
  - **description** â†’ optional text about the DAG
  - **tasks** â†’ individual units of work (PythonOperator, BashOperator, etc.)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2ï¸âƒ£ DEFAULT_ARGS
# Default arguments
MANDATORY / COMMON FIELDS IN default_args

| Field                     | Description                                                                 | Required?         | Example Value                           |
|----------------------------|-----------------------------------------------------------------------------|-----------------|-----------------------------------------|
| owner                      | Owner of the DAG / task. Typically your team or user name.                  | âœ… Recommended   | "data_eng"                              |
| start_date(**Mandatory**)  | The start date for the DAG (when scheduling begins).                        | âœ… Mandatory     | datetime(2025, 1, 1)                    |
| depends_on_past            | Whether the task depends on the previous DAG runâ€™s success.                 | Optional         | False                                    |
| retries                    | Number of times to retry a task if it fails.                                | Optional         | 1                                        |
| retry_delay                | Delay between retries.                                                      | Optional         | timedelta(minutes=5)                     |
| email                      | List of emails for alerts.                                                 | Optional         | ["alerts@example.com"]                   |
| email_on_failure           | Send email when task fails.                                                | Optional         | True                                     |
| email_on_retry             | Send email when task retries.                                              | Optional         | False                                    |
| priority_weight            | Task priority relative to others.                                           | Optional         | 10                                       |
| queue                      | Name of the queue for task execution (if using CeleryExecutor).             | Optional         | "default"                                |

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
3ï¸âƒ£ EXAMPLE default_args
**1. PyhonOperator**
```python
from datetime import datetime, timedelta

default_args = {
    "owner": "data_eng",
    "start_date": datetime(2025, 1, 1),
    "depends_on_past": False,
    "email": ["alerts@example.com"],
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5)
}

# Define DAG
dag = DAG(
    dag_id="etl_example_dag",
    default_args=default_args,
    description="Example ETL DAG workflow",
    schedule_interval="@daily",
    schedule_interval=timedelta(days=1)
)

# Define tasks
def extract(): print("Extracting data from S3...")
def transform(): print("Transforming data...")
def load(): print("Loading data into Redshift...")

task_extract = PythonOperator(task_id="extract_task", python_callable=extract, dag=dag)
task_transform = PythonOperator(task_id="transform_task", python_callable=transform, dag=dag)
task_load = PythonOperator(task_id="load_task", python_callable=load, dag=dag)

# Set task dependencies (workflow order)
task_extract >> task_transform >> task_load
```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4ï¸âƒ£ HOW IT WORKS
1. DAG parses tasks and dependencies.
2. Scheduler triggers DAG according to schedule_interval.
3. task_extract runs first â†’ task_transform â†’ task_load.
4. Metadata DB stores states; retries and email alerts happen automatically if a task fails.
5. Web UI shows DAG graph, task logs, and execution history.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… SUMMARY
â€¢ DAG = workflow definition; tasks are units of work.

â€¢ default_args centralizes common task parameters.

â€¢ schedule_interval determines execution frequency.

â€¢ Dependencies define execution order (>> or set_upstream/set_downstream).


â€¢ Airflow automatically handles retries, alerts, and logging.

**2. BashOperator Example**

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator

# Default arguments for DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': ['admin@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Instantiate the DAG
dag = DAG(
    'simple_bash_dag',
    default_args=default_args,
    description='A simple Bash DAG example',
    schedule_interval=timedelta(days=1),  # daily DAG
    start_date=datetime(2025, 10, 1),
    catchup=False,
)

# Define tasks
task1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag,
)

task2 = BashOperator(
    task_id='sleep',
    bash_command='sleep 5',
    dag=dag,
)

task3 = BashOperator(
    task_id='echo_hello',
    bash_command='echo "Hello Airflow!"',
    dag=dag,
)

# Set dependencies
task1 >> task2 >> task3
```

**Explanation:**

task1 >> task2 >> task3 â†’ task1 runs first, then task2, then task3.

catchup=False â†’ skips backfilling old dates.

schedule_interval=timedelta(days=1) â†’ runs daily.

**Branching DAGs**

**3. BranchPythonOperator**
```python
from airflow.operators.python import BranchPythonOperator

def choose_path():
    from random import choice
    return choice(['task_a', 'task_b'])

branch_task = BranchPythonOperator(
    task_id='branching',
    python_callable=choose_path,
    dag=dag,
)

task_a = BashOperator(
    task_id='task_a',
    bash_command='echo "Task A"',
    dag=dag,
)

task_b = BashOperator(
    task_id='task_b',
    bash_command='echo "Task B"',
    dag=dag,
)

task3 >> branch_task
branch_task >> [task_a, task_b]

```

**4. XCom (Passing Data Between Tasks)**
XCom allows tasks to exchange messages.

```python
from airflow.operators.python import PythonOperator

def push_value(ti):
    ti.xcom_push(key='sample_key', value=42)

def pull_value(ti):
    value = ti.xcom_pull(key='sample_key', task_ids='push_task')
    print(f"Pulled value: {value}")

push_task = PythonOperator(
    task_id='push_task',
    python_callable=push_value,
    dag=dag,
)

pull_task = PythonOperator(
    task_id='pull_task',
    python_callable=pull_value,
    dag=dag,
)

push_task >> pull_task

```
