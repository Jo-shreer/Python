📘 AIRFLOW DAG STRUCTURE & default_args

1️⃣ DAG STRUCTURE
• A DAG in Airflow is a **Python object** that defines a workflow.
• It contains:
  - **dag_id** → unique identifier for the DAG
  - **default_args** → dictionary of default task parameters
  - **schedule_interval** → defines how often the DAG runs
  - **description** → optional text about the DAG
  - **tasks** → individual units of work (PythonOperator, BashOperator, etc.)

──────────────────────────────
2️⃣ DEFAULT_ARGS
# Default arguments
MANDATORY / COMMON FIELDS IN default_args

| Field                     | Description                                                                 | Required?         | Example Value                           |
|----------------------------|-----------------------------------------------------------------------------|-----------------|-----------------------------------------|
| owner                      | Owner of the DAG / task. Typically your team or user name.                  | ✅ Recommended   | "data_eng"                              |
| start_date(**Mandatory**)  | The start date for the DAG (when scheduling begins).                        | ✅ Mandatory     | datetime(2025, 1, 1)                    |
| depends_on_past            | Whether the task depends on the previous DAG run’s success.                 | Optional         | False                                    |
| retries                    | Number of times to retry a task if it fails.                                | Optional         | 1                                        |
| retry_delay                | Delay between retries.                                                      | Optional         | timedelta(minutes=5)                     |
| email                      | List of emails for alerts.                                                 | Optional         | ["alerts@example.com"]                   |
| email_on_failure           | Send email when task fails.                                                | Optional         | True                                     |
| email_on_retry             | Send email when task retries.                                              | Optional         | False                                    |
| priority_weight            | Task priority relative to others.                                           | Optional         | 10                                       |
| queue                      | Name of the queue for task execution (if using CeleryExecutor).             | Optional         | "default"                                |

──────────────────────────────
3️⃣ EXAMPLE default_args

from datetime import datetime, timedelta

default_args = {
    "owner": "data_eng",
    "start_date": datetime(2025, 1, 1),
    "depends_on_past": False,
    "email": ["alerts@example.com"],
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5)
}

# Define DAG
dag = DAG(
    dag_id="etl_example_dag",
    default_args=default_args,
    description="Example ETL DAG workflow",
    schedule_interval="@daily"
)

# Define tasks
def extract(): print("Extracting data from S3...")
def transform(): print("Transforming data...")
def load(): print("Loading data into Redshift...")

task_extract = PythonOperator(task_id="extract_task", python_callable=extract, dag=dag)
task_transform = PythonOperator(task_id="transform_task", python_callable=transform, dag=dag)
task_load = PythonOperator(task_id="load_task", python_callable=load, dag=dag)

# Set task dependencies (workflow order)
task_extract >> task_transform >> task_load

──────────────────────────────
4️⃣ HOW IT WORKS
1. DAG parses tasks and dependencies.
2. Scheduler triggers DAG according to schedule_interval.
3. task_extract runs first → task_transform → task_load.
4. Metadata DB stores states; retries and email alerts happen automatically if a task fails.
5. Web UI shows DAG graph, task logs, and execution history.

──────────────────────────────
✅ SUMMARY
• DAG = workflow definition; tasks are units of work.
• default_args centralizes common task parameters.
• schedule_interval determines execution frequency.
• Dependencies define execution order (>> or set_upstream/set_downstream).
• Airflow automatically handles retries, alerts, and logging.
