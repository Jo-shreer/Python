ðŸ“˜ AIRFLOW DAG STRUCTURE & default_args

1ï¸âƒ£ DAG STRUCTURE
â€¢ A DAG in Airflow is a **Python object** that defines a workflow.
â€¢ It contains:
  - **dag_id** â†’ unique identifier for the DAG
  - **default_args** â†’ dictionary of default task parameters
  - **schedule_interval** â†’ defines how often the DAG runs
  - **description** â†’ optional text about the DAG
  - **tasks** â†’ individual units of work (PythonOperator, BashOperator, etc.)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2ï¸âƒ£ DEFAULT_ARGS
â€¢ A Python dictionary used to define **common parameters** for all tasks:
  - owner â†’ workflow owner
  - start_date â†’ when DAG starts
  - retries â†’ number of retries if task fails
  - retry_delay â†’ time between retries
  - email â†’ alert emails
  - depends_on_past â†’ if task depends on previous run
â€¢ Tasks inherit these values unless overridden.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
3ï¸âƒ£ EXAMPLE DAG

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

# Default arguments
default_args = {
    "owner": "data_eng",
    "start_date": datetime(2025, 1, 1),
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": True,
    "depends_on_past": False
}

# Define DAG
dag = DAG(
    dag_id="etl_example_dag",
    default_args=default_args,
    description="Example ETL DAG workflow",
    schedule_interval="@daily"
)

# Define tasks
def extract(): print("Extracting data from S3...")
def transform(): print("Transforming data...")
def load(): print("Loading data into Redshift...")

task_extract = PythonOperator(task_id="extract_task", python_callable=extract, dag=dag)
task_transform = PythonOperator(task_id="transform_task", python_callable=transform, dag=dag)
task_load = PythonOperator(task_id="load_task", python_callable=load, dag=dag)

# Set task dependencies (workflow order)
task_extract >> task_transform >> task_load

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4ï¸âƒ£ HOW IT WORKS
1. DAG parses tasks and dependencies.
2. Scheduler triggers DAG according to schedule_interval.
3. task_extract runs first â†’ task_transform â†’ task_load.
4. Metadata DB stores states; retries and email alerts happen automatically if a task fails.
5. Web UI shows DAG graph, task logs, and execution history.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… SUMMARY
â€¢ DAG = workflow definition; tasks are units of work.
â€¢ default_args centralizes common task parameters.
â€¢ schedule_interval determines execution frequency.
â€¢ Dependencies define execution order (>> or set_upstream/set_downstream).
â€¢ Airflow automatically handles retries, alerts, and logging.
