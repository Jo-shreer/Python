ðŸ“˜ AIRFLOW DAG STRUCTURE & default_args

1ï¸âƒ£ DAG STRUCTURE
â€¢ A DAG in Airflow is a **Python object** that defines a workflow.
â€¢ It contains:
  - **dag_id** â†’ unique identifier for the DAG
  - **default_args** â†’ dictionary of default task parameters
  - **schedule_interval** â†’ defines how often the DAG runs
  - **description** â†’ optional text about the DAG
  - **tasks** â†’ individual units of work (PythonOperator, BashOperator, etc.)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2ï¸âƒ£ DEFAULT_ARGS
# Default arguments
MANDATORY / COMMON FIELDS IN default_args

| Field                     | Description                                                                 | Required?         | Example Value                           |
|----------------------------|-----------------------------------------------------------------------------|-----------------|-----------------------------------------|
| owner                      | Owner of the DAG / task. Typically your team or user name.                  | âœ… Recommended   | "data_eng"                              |
| start_date(**Mandatory**)  | The start date for the DAG (when scheduling begins).                        | âœ… Mandatory     | datetime(2025, 1, 1)                    |
| depends_on_past            | Whether the task depends on the previous DAG runâ€™s success.                 | Optional         | False                                    |
| retries                    | Number of times to retry a task if it fails.                                | Optional         | 1                                        |
| retry_delay                | Delay between retries.                                                      | Optional         | timedelta(minutes=5)                     |
| email                      | List of emails for alerts.                                                 | Optional         | ["alerts@example.com"]                   |
| email_on_failure           | Send email when task fails.                                                | Optional         | True                                     |
| email_on_retry             | Send email when task retries.                                              | Optional         | False                                    |
| priority_weight            | Task priority relative to others.                                           | Optional         | 10                                       |
| queue                      | Name of the queue for task execution (if using CeleryExecutor).             | Optional         | "default"                                |

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
3ï¸âƒ£ EXAMPLE default_args

from datetime import datetime, timedelta

default_args = {
    "owner": "data_eng",
    "start_date": datetime(2025, 1, 1),
    "depends_on_past": False,
    "email": ["alerts@example.com"],
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5)
}

# Define DAG
dag = DAG(
    dag_id="etl_example_dag",
    default_args=default_args,
    description="Example ETL DAG workflow",
    schedule_interval="@daily"
)

# Define tasks
def extract(): print("Extracting data from S3...")
def transform(): print("Transforming data...")
def load(): print("Loading data into Redshift...")

task_extract = PythonOperator(task_id="extract_task", python_callable=extract, dag=dag)
task_transform = PythonOperator(task_id="transform_task", python_callable=transform, dag=dag)
task_load = PythonOperator(task_id="load_task", python_callable=load, dag=dag)

# Set task dependencies (workflow order)
task_extract >> task_transform >> task_load

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4ï¸âƒ£ HOW IT WORKS
1. DAG parses tasks and dependencies.
2. Scheduler triggers DAG according to schedule_interval.
3. task_extract runs first â†’ task_transform â†’ task_load.
4. Metadata DB stores states; retries and email alerts happen automatically if a task fails.
5. Web UI shows DAG graph, task logs, and execution history.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… SUMMARY
â€¢ DAG = workflow definition; tasks are units of work.
â€¢ default_args centralizes common task parameters.
â€¢ schedule_interval determines execution frequency.
â€¢ Dependencies define execution order (>> or set_upstream/set_downstream).
â€¢ Airflow automatically handles retries, alerts, and logging.
