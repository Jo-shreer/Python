📘 AIRFLOW DAG STRUCTURE & default_args

1️⃣ DAG STRUCTURE
• A DAG in Airflow is a **Python object** that defines a workflow.
• It contains:
  - **dag_id** → unique identifier for the DAG
  - **default_args** → dictionary of default task parameters
  - **schedule_interval** → defines how often the DAG runs
  - **description** → optional text about the DAG
  - **tasks** → individual units of work (PythonOperator, BashOperator, etc.)

──────────────────────────────
2️⃣ DEFAULT_ARGS
# Default arguments
MANDATORY / COMMON FIELDS IN default_args

| Field                     | Description                                                                 | Required?         | Example Value                           |
|----------------------------|-----------------------------------------------------------------------------|-----------------|-----------------------------------------|
| owner                      | Owner of the DAG / task. Typically your team or user name.                  | ✅ Recommended   | "data_eng"                              |
| start_date(**Mandatory**)  | The start date for the DAG (when scheduling begins).                        | ✅ Mandatory     | datetime(2025, 1, 1)                    |
| depends_on_past            | Whether the task depends on the previous DAG run’s success.                 | Optional         | False                                    |
| retries                    | Number of times to retry a task if it fails.                                | Optional         | 1                                        |
| retry_delay                | Delay between retries.                                                      | Optional         | timedelta(minutes=5)                     |
| email                      | List of emails for alerts.                                                 | Optional         | ["alerts@example.com"]                   |
| email_on_failure           | Send email when task fails.                                                | Optional         | True                                     |
| email_on_retry             | Send email when task retries.                                              | Optional         | False                                    |
| priority_weight            | Task priority relative to others.                                           | Optional         | 10                                       |
| queue                      | Name of the queue for task execution (if using CeleryExecutor).             | Optional         | "default"                                |

──────────────────────────────
3️⃣ EXAMPLE default_args
**1. PyhonOperator**
```python
from datetime import datetime, timedelta

default_args = {
    "owner": "data_eng",
    "start_date": datetime(2025, 1, 1),
    "depends_on_past": False,
    "email": ["alerts@example.com"],
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5)
}

# Define DAG
dag = DAG(
    dag_id="etl_example_dag",
    default_args=default_args,
    description="Example ETL DAG workflow",
    schedule_interval="@daily",
    schedule_interval=timedelta(days=1)
)

# Define tasks
def extract(): print("Extracting data from S3...")
def transform(): print("Transforming data...")
def load(): print("Loading data into Redshift...")

task_extract = PythonOperator(task_id="extract_task", python_callable=extract, dag=dag)
task_transform = PythonOperator(task_id="transform_task", python_callable=transform, dag=dag)
task_load = PythonOperator(task_id="load_task", python_callable=load, dag=dag)

# Set task dependencies (workflow order)
task_extract >> task_transform >> task_load
```
──────────────────────────────
4️⃣ HOW IT WORKS
1. DAG parses tasks and dependencies.
2. Scheduler triggers DAG according to schedule_interval.
3. task_extract runs first → task_transform → task_load.
4. Metadata DB stores states; retries and email alerts happen automatically if a task fails.
5. Web UI shows DAG graph, task logs, and execution history.

──────────────────────────────
✅ SUMMARY
• DAG = workflow definition; tasks are units of work.

• default_args centralizes common task parameters.

• schedule_interval determines execution frequency.

• Dependencies define execution order (>> or set_upstream/set_downstream).


• Airflow automatically handles retries, alerts, and logging.

**2. BashOperator Example**

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator

# Default arguments for DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': ['admin@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Instantiate the DAG
dag = DAG(
    'simple_bash_dag',
    default_args=default_args,
    description='A simple Bash DAG example',
    schedule_interval=timedelta(days=1),  # daily DAG
    start_date=datetime(2025, 10, 1),
    catchup=False,
)

# Define tasks
task1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag,
)

task2 = BashOperator(
    task_id='sleep',
    bash_command='sleep 5',
    dag=dag,
)

task3 = BashOperator(
    task_id='echo_hello',
    bash_command='echo "Hello Airflow!"',
    dag=dag,
)

# Set dependencies
task1 >> task2 >> task3
```

**Explanation:**

task1 >> task2 >> task3 → task1 runs first, then task2, then task3.

catchup=False → skips backfilling old dates.

schedule_interval=timedelta(days=1) → runs daily.

**Branching DAGs**

**3. BranchPythonOperator**
```python
from airflow.operators.python import BranchPythonOperator

def choose_path():
    from random import choice
    return choice(['task_a', 'task_b'])

branch_task = BranchPythonOperator(
    task_id='branching',
    python_callable=choose_path,
    dag=dag,
)

task_a = BashOperator(
    task_id='task_a',
    bash_command='echo "Task A"',
    dag=dag,
)

task_b = BashOperator(
    task_id='task_b',
    bash_command='echo "Task B"',
    dag=dag,
)

task3 >> branch_task
branch_task >> [task_a, task_b]

```

**4. XCom (Passing Data Between Tasks)**
XCom allows tasks to exchange messages.

```python
from airflow.operators.python import PythonOperator

def push_value(ti):
    ti.xcom_push(key='sample_key', value=42)

def pull_value(ti):
    value = ti.xcom_pull(key='sample_key', task_ids='push_task')
    print(f"Pulled value: {value}")

push_task = PythonOperator(
    task_id='push_task',
    python_callable=push_value,
    dag=dag,
)

pull_task = PythonOperator(
    task_id='pull_task',
    python_callable=pull_value,
    dag=dag,
)

push_task >> pull_task

```
🔍 STEP-BY-STEP EXPLANATION

1️⃣ PythonOperator
• Used to run a Python function as an Airflow task.
• Arguments:
  - task_id → unique identifier for the task
  - python_callable → Python function to execute
  - dag → DAG object the task belongs to

2️⃣ XCom (Cross-Communication)
• Airflow’s mechanism to **share data between tasks**.
• Each task can **push** data with `xcom_push()` and **pull** data with `xcom_pull()`.
• Commonly used for small pieces of data (numbers, strings, JSON), not for large datasets.

3️⃣ push_value(ti)
• `ti` = task instance automatically passed by Airflow (via PythonOperator).
• `ti.xcom_push(key='sample_key', value=42)` → stores the value `42` with key `sample_key` for this task.
• This value can later be retrieved by another task.

4️⃣ pull_value(ti)
• `ti.xcom_pull(key='sample_key', task_ids='push_task')` → fetches the value pushed by `push_task`.
• Prints: `Pulled value: 42`.

5️⃣ Task Dependencies
• `push_task >> pull_task` → sets the execution order.
  - `push_task` runs first (pushes value to XCom)
  - `pull_task` runs next (pulls value from XCom and prints it)

✅ EXECUTION FLOW
1. Scheduler triggers DAG.
2. push_task executes:
   - Calls push_value(ti)
   - Pushes 42 to XCom with key 'sample_key'
3. pull_task executes after push_task:
   - Calls pull_value(ti)
   - Pulls 42 from XCom
   - Prints "Pulled value: 42"
4. DAG completes successfully.
```
** Expected Console / Log Output of pull_task:**
[2025-10-13 10:00:00,000] {python_operator.py:114} INFO - Running function pull_value

Pulled value: 42

[2025-10-13 10:00:01,000] {python_operator.py:120} INFO - Done executing function pull_value

🧠 SUMMARY
• PythonOperator allows executing Python functions in a DAG.  
• XCom is used for **task-to-task communication** in Airflow.  
• `ti.xcom_push()` → push value; `ti.xcom_pull()` → pull value.  
• Task order is defined using `>>` or `set_upstream/set_downstream`.  
• This mechanism is ideal for small messages, parameters, or signals between tasks.
````
**5. Airflow & Spark (AWS/EMR Integration)**
You can run Spark jobs using SparkSubmitOperator:
```python
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

spark_task = SparkSubmitOperator(
    task_id='spark_job',
    application='/path/to/spark_job.py',
    conn_id='spark_default',
    executor_memory='2g',
    total_executor_cores=2,
    dag=dag
)

task3 >> spark_task
```

