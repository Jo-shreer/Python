📘 AIRFLOW DAG STRUCTURE & default_args

1️⃣ DAG STRUCTURE
• A DAG in Airflow is a **Python object** that defines a workflow.
• It contains:
  - **dag_id** → unique identifier for the DAG
  - **default_args** → dictionary of default task parameters
  - **schedule_interval** → defines how often the DAG runs
  - **description** → optional text about the DAG
  - **tasks** → individual units of work (PythonOperator, BashOperator, etc.)

──────────────────────────────
2️⃣ DEFAULT_ARGS
• A Python dictionary used to define **common parameters** for all tasks:
  - owner → workflow owner
  - start_date → when DAG starts
  - retries → number of retries if task fails
  - retry_delay → time between retries
  - email → alert emails
  - depends_on_past → if task depends on previous run
• Tasks inherit these values unless overridden.

──────────────────────────────
3️⃣ EXAMPLE DAG

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

# Default arguments
default_args = {
    "owner": "data_eng",
    "start_date": datetime(2025, 1, 1),
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": True,
    "depends_on_past": False
}

# Define DAG
dag = DAG(
    dag_id="etl_example_dag",
    default_args=default_args,
    description="Example ETL DAG workflow",
    schedule_interval="@daily"
)

# Define tasks
def extract(): print("Extracting data from S3...")
def transform(): print("Transforming data...")
def load(): print("Loading data into Redshift...")

task_extract = PythonOperator(task_id="extract_task", python_callable=extract, dag=dag)
task_transform = PythonOperator(task_id="transform_task", python_callable=transform, dag=dag)
task_load = PythonOperator(task_id="load_task", python_callable=load, dag=dag)

# Set task dependencies (workflow order)
task_extract >> task_transform >> task_load

──────────────────────────────
4️⃣ HOW IT WORKS
1. DAG parses tasks and dependencies.
2. Scheduler triggers DAG according to schedule_interval.
3. task_extract runs first → task_transform → task_load.
4. Metadata DB stores states; retries and email alerts happen automatically if a task fails.
5. Web UI shows DAG graph, task logs, and execution history.

──────────────────────────────
✅ SUMMARY
• DAG = workflow definition; tasks are units of work.
• default_args centralizes common task parameters.
• schedule_interval determines execution frequency.
• Dependencies define execution order (>> or set_upstream/set_downstream).
• Airflow automatically handles retries, alerts, and logging.
