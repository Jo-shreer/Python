## What is Airflow
Airflow is a workflow orchestration engine that runs DAGs (Directed Acyclic Graphs).
DAG = the definition of tasks and dependencies.
Tasks are instances of Operators (BashOperator, PythonOperator, SparkSubmitOperator, etc.).
Scheduler decides when to run DAG runs; Executor runs tasks.
UI shows DAG runs, task logs, retries, SLA, etc.

## 2 — Install & run - Option A — Quick local install with pip (Airflow 2.x)
```bash
# create venv (recommended)
python -m venv .venv && source .venv/bin/activate

# pick a constraint file matching your python version from airflow.apache.org,
# but for quick demo you can:
pip install "apache-airflow==2.6.3" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.6.3/constraints-3.10.txt"

# initialize DB
airflow db init

# create user
airflow users create \
  --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com

# start webserver (background or new terminal)
airflow webserver --port 8080

# start scheduler (in separate terminal)
airflow scheduler

# Open UI at http://localhost:8080.
```
## Option B — Docker Compose (recommended for parity)
## Use the official Airflow image and docker-compose.yaml. Minimal example:
```yaml
# docker-compose-airflow.yml (very small example)

version: "3.8"
services:
  airflow-postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD","pg_isready","-U","airflow"]
  airflow-webserver:
    image: apache/airflow:2.6.3
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    depends_on:
      airflow-postgres:
        condition: service_healthy

# Then docker-compose -f docker-compose-airflow.yml up and open http://localhost:8080.
```
## 3 — Anatomy of a DAG (basic)
```python

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator

default_args = {
    "owner": "you",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="simple_bash_dag",
    default_args=default_args,
    description="Simple DAG example",
    schedule_interval="@daily",
    start_date=datetime(2025, 10, 1),
    catchup=False,
) as dag:

    t1 = BashOperator(task_id="print_date", bash_command="date")
    t2 = BashOperator(task_id="sleep", bash_command="sleep 5")
    t3 = BashOperator(task_id="echo", bash_command='echo "Done"')

    t1 >> t2 >> t3

# Place file in $AIRFLOW_HOME/dags (or ./dags when using Docker Compose bind mount).
# In UI you’ll see simple_bash_dag. Trigger it manually or wait for scheduler.
```
## TaskFlow API (recommended for new code)
```python

from airflow.decorators import dag, task

@dag(start_date=datetime(2025,10,1), schedule_interval=None, catchup=False)
def my_etl():
    @task
    def extract():
        return [1,2,3]

    @task
    def transform(numbers):
        return [n*2 for n in numbers]

    @task
    def load(result):
        print(result)

    load(transform(extract()))

dag = my_etl()

```
## SparkSubmitOperator (run Spark)
```python

from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

spark_task = SparkSubmitOperator(
    task_id="spark_job",
    application="/opt/airflow/dags/scripts/my_spark_job.py",
    conn_id="spark_default",
    application_args=["--input","s3://bucket/raw/","--output","s3://bucket/processed/"]
)

#Example: Corresponding Spark Script
#Let’s look at what /opt/airflow/dags/scripts/my_spark_job.py might look like

from pyspark.sql import SparkSession
import argparse

# --- 1️⃣ Parse CLI arguments ---
parser = argparse.ArgumentParser()
parser.add_argument("--input", required=True)
parser.add_argument("--output", required=True)
args = parser.parse_args()

# --- 2️⃣ Create Spark session ---
spark = SparkSession.builder.appName("AirflowETLJob").getOrCreate()

# --- 3️⃣ Read data from S3 ---
df = spark.read.option("header", True).csv(args.input)

# --- 4️⃣ Transform data ---
df_transformed = df.filter(df["status"] == "ACTIVE")

# --- 5️⃣ Write back to S3 ---
df_transformed.write.mode("overwrite").parquet(args.output)

spark.stop()

#So when Airflow runs the DAG, this task executes roughly this command:

 spark-submit \
  --master yarn \
  /opt/airflow/dags/scripts/my_spark_job.py \
  --input s3://bucket/raw/ \
  --output s3://bucket/processed/

# Real-World Example in a DAG
from datetime import datetime
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

with DAG(
    dag_id="spark_etl_dag",
    start_date=datetime(2025, 10, 1),
    schedule_interval="@daily",
    catchup=False,
) as dag:

    transform_data = SparkSubmitOperator(
        task_id="transform_sales_data",
        application="/opt/airflow/dags/scripts/sales_transform.py",
        conn_id="spark_default",
        application_args=[
            "--input", "s3://my-bucket/raw/sales/{{ ds }}/",
            "--output", "s3://my-bucket/processed/sales/{{ ds }}/"
        ],
    )

transform_data
#This will:
#Wait for the DAG’s schedule (daily)
#Run your Spark script on the cluster defined in spark_default
#Pass input and output paths dynamically (using templating {{ ds }} = date string)

```
## PostgresOperator / Redshift (run SQL)
```python

from airflow.providers.postgres.operators.postgres import PostgresOperator

load_task = PostgresOperator(
    task_id="create_table",
    postgres_conn_id="redshift_conn",  # connection configured in Airflow UI
    sql="""
    CREATE TABLE IF NOT EXISTS analytics.sales_summary (
      region varchar(50),
      total_sales numeric
    );
    """,
)

```
## AWS Glue operator (trigger Glue job)
```python
from airflow.providers.amazon.aws.operators.glue import AwsGlueJobOperator

glue_task = AwsGlueJobOperator(
    task_id="start_glue_job",
    job_name="my-glue-job",
    script_args={"--s3_input": "s3://my-bucket/raw/"},
    aws_conn_id="aws_default",
)


```
## 5 — Sensors, XCom, Branching - Sensor example (wait for S3 file)
```python
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor

wait_for_file = S3KeySensor(
    task_id="wait_for_file",
    bucket_key="raw/{{ ds }}/*.csv",
    bucket_name="my-bucket",
    aws_conn_id="aws_default",
    poke_interval=60,  # seconds
    timeout=60*30,
)

```
## XCom (pass data between tasks)
```python

def push(ti):
    ti.xcom_push("my_key", {"count": 100})

def pull(ti):
    val = ti.xcom_pull(key="my_key", task_ids="push_task")
    print(val)

push_task = PythonOperator(task_id="push_task", python_callable=push)
pull_task = PythonOperator(task_id="pull_task", python_callable=pull)
push_task >> pull_task

```
## Branching (BranchPythonOperator)
```python
from airflow.operators.python import BranchPythonOperator

def choose(**kwargs):
    return "task_a" if condition else "task_b"

branch = BranchPythonOperator(
    task_id="branching",
    python_callable=choose,
)

```
## 6 — Connections, Variables, Secrets, Templating
## Connections (UI): Admin → Connections
Define aws_default, spark_default, redshift_conn, etc.
A connection stores host/port/user/pass and extra JSON (like role ARN).

## Variables (UI): Admin → Variables
Small key/value store for environment-like settings. Use Variable.get("key", default_var=...).

## Secrets Backends (recommended)
Use HashiCorp Vault, AWS Secrets Manager, or environment variables rather than hardcoding credentials.
Configure AIRFLOW__SECRETS__BACKENDS in airflow.cfg or env vars.

## Jinja templating in Operators
Operators allow templated params using {{ ds }}, {{ ts }} etc.
```python

BashOperator(task_id="echo", bash_command="echo {{ ds }}")

```
## 7 — Scheduling, catchup, retries, SLA
schedule_interval can be @daily, cron, or None (manual).
start_date is important: Airflow schedules DAG runs for the interval end.
catchup=True will run backfill jobs for missed intervals — default can be surprising; set catchup=False if you don't want that.

Retries:
```python

default_args = {"retries": 3, "retry_delay": timedelta(minutes=5)}
from airflow.utils.task_group import TaskGroup
task.sla = timedelta(hours=1)  # if a task run exceeds SLA, an SLA miss is recorded

```
SLA:
```python

from airflow.utils.task_group import TaskGroup
task.sla = timedelta(hours=1)  # if a task run exceeds SLA, an SLA miss is recorded

```
## 8 — Testing & Debugging
Test a DAG locally (no scheduler)
```bash
# run a single task (simulate)
airflow tasks test simple_bash_dag print_date 2025-10-14

```
## What This Command Does
This command tests a single task (not the whole DAG) in isolation, as if it were run on a specific execution date.
It does not trigger the whole DAG, nor does it affect the Airflow scheduler, database, or other tasks.
It’s used to debug or test one task manually — like a “dry run”.

# Command Structure
| **Part**             | **Meaning**                                       |
| -------------------- | ------------------------------------------------- |
| `airflow tasks test` | Airflow CLI command to run one task for debugging |
| `simple_bash_dag`    | The **DAG ID** where the task lives               |
| `print_date`         | The **task_id** you want to test                  |
| `2025-10-14`         | The **execution date** (logical date) to simulate |

# example from airflow import DAG
```python
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    dag_id='simple_bash_dag',
    start_date=datetime(2025, 10, 1),
    schedule_interval='@daily',
    catchup=False,
) as dag:

    print_date = BashOperator(
        task_id='print_date',
        bash_command='date'
    )

    echo_hello = BashOperator(
        task_id='echo_hello',
        bash_command='echo "Hello from Airflow!"'
    )

    print_date >> echo_hello

#In this DAG, there are two tasks:
#print_date → prints the current date
#echo_hello → prints a message

airflow tasks test simple_bash_dag print_date 2025-10-14

Airflow will:
Load the DAG definition (simple_bash_dag) from your dags/ folder
Find the task named print_date
Set the logical execution date to 2025-10-14
Run that task immediately (bypassing scheduler or dependencies)
Print logs and results to the console
Not mark it as success/failure in the Airflow database — it’s just for local testing

op
[2025-10-14 00:00:00,000] {taskinstance.py:1120} INFO - Running task: print_date
[2025-10-14 00:00:00,001] {bash_operator.py:158} INFO - Running command: date
Tue Oct 14 00:00:00 UTC 2025
[2025-10-14 00:00:00,002] {taskinstance.py:1254} INFO - Task exited with return code 0


```

## Compare with Other Commands
| **Command**                                    | **Purpose**                                                    |
| ---------------------------------------------- | -------------------------------------------------------------- |
| `airflow dags trigger <dag_id>`                | Triggers a full DAG run (includes all tasks)                   |
| `airflow tasks test <dag_id> <task_id> <date>` | Runs a single task manually                                    |
| `airflow tasks run <dag_id> <task_id> <date>`  | Runs a task and records it in Airflow metadata (unlike `test`) |


## Key Difference: test vs run
| **Command**          | **Writes to DB?** | **Triggers Dependencies?** | **Use Case**              |
| -------------------- | ----------------- | -------------------------- | ------------------------- |
| `airflow tasks test` | ❌ No              | ❌ No                       | For debugging a task      |
| `airflow tasks run`  | ✅ Yes             | ✅ Yes                      | For forcing an actual run |

## Unit tests with pytest
```python
# tests/test_dag.py

from airflow.models.dagbag import DagBag

def test_dag_import():
    dagbag = DagBag(dag_folder="dags", include_examples=False)
    assert dagbag.import_errors == {}
    assert "simple_bash_dag" in dagbag.dags

```
## 9 — Monitoring & Alerts
Configure email alerts in airflow.cfg or per-task email_on_failure=True.
Use external monitoring: Prometheus + Grafana exporters for Airflow metrics.
Use SLA callbacks or sensors to enforce SLAs.

## 10 — Best Practices
Keep DAGs idempotent (re-running should be safe).
Keep tasks small and single-purpose (micro-tasks).
Use TaskGroup to group related tasks.
Avoid heavy compute in PythonOperator — use external services (Spark, EMR, ECS)
Use params and Variables instead of hardcoding configuration.
Keep secrets out of code (use Secrets Backend).
Use XCom sparingly for small messages — not for big payloads (use S3 instead).
Pin provider versions and use constraints file for reproducible installs.

## 11 — Executors & Deployment options
LocalExecutor: good for dev, single machine parallelism.
CeleryExecutor: scaling with worker processes (requires broker like Redis/RabbitMQ).
KubernetesExecutor: each task runs in its own pod (great for cloud-native).
KubernetesPodOperator: run tasks as pods from within DAGs (for containerized workloads).

## Example: Small ETL DAG (S3 → Spark → Redshift)
```python

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator

default_args = {
    "owner": "dataeng",
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="s3_spark_redshift",
    default_args=default_args,
    schedule_interval="@daily",
    start_date=datetime(2025,10,1),
    catchup=False,
) as dag:

    wait = S3KeySensor(
        task_id="wait_for_raw",
        bucket_key="raw/{{ ds }}/*.csv",
        bucket_name="my-bucket",
        aws_conn_id="aws_default",
        poke_interval=60,
        timeout=60*30,
    )

    spark = SparkSubmitOperator(
        task_id="transform_with_spark",
        application="/opt/airflow/dags/scripts/transform.py",
        conn_id="spark_default",
        application_args=["--input", "s3://my-bucket/raw/{{ ds }}/", "--output","s3://my-bucket/processed/{{ ds }}/"],
    )

    load = PostgresOperator(
        task_id="load_to_redshift",
        postgres_conn_id="redshift_conn",
        sql="COPY analytics.table FROM 's3://my-bucket/processed/{{ ds }}/' iam_role 'arn:aws:iam::123:role/RedshiftCopyRole' FORMAT AS PARQUET;",
    )

    wait >> spark >> load

```
## 15 — Handy Cheatsheet (quick references)
Initialize DB: airflow db init
Create user: airflow users create ...
Start webserver: airflow webserver --port 8080
Start scheduler: airflow scheduler
Test task: airflow tasks test <dag_id> <task_id> <execution_date>
List DAGs: airflow dags list
Pause DAG: airflow dags pause <dag_id>
Unpause DAG: airflow dags unpause <dag_id>

## ETL DAG that uses TaskFlow API + S3 + Glue + Redshift and show how to configure Connections/Variables
# Full ETL Pipeline DAG — TaskFlow API + S3 + Glue + Redshift (Airflow 2.x)

This DAG performs an end-to-end ETL flow:
1️⃣ **Extract** → Read data from an S3 bucket  
2️⃣ **Transform** → Trigger an AWS Glue job for cleaning/enrichment  
3️⃣ **Load** → Copy the transformed data into Redshift  
4️⃣ Uses Airflow **TaskFlow API**, AWS Connections, and Variables  

---

## Prerequisites

### 1. Airflow Providers
Install the necessary providers:
```bash
pip install apache-airflow-providers-amazon apache-airflow-providers-postgres
```
## 2. Airflow Connections (set in UI → Admin → Connections)
| Conn ID         | Type                | Description                                              |
| --------------- | ------------------- | -------------------------------------------------------- |
| `aws_default`   | Amazon Web Services | Use IAM Role / Access Keys                               |
| `redshift_conn` | Postgres            | Host, Port (5439), User, Password, Database for Redshift |

## 3. Airflow Variables (UI → Admin → Variables)
| Key                   | Example Value             | Description                 |
| --------------------- | ------------------------- | --------------------------- |
| `s3_raw_bucket`       | `my-raw-bucket`           | Bucket for raw input        |
| `s3_processed_bucket` | `my-processed-bucket`     | Bucket for transformed data |
| `glue_job_name`       | `etl-transform-job`       | Glue job to trigger         |
| `redshift_table`      | `analytics.sales_cleaned` | Target Redshift table       |

## DAG: dags/etl_taskflow_s3_glue_redshift.py
```python

from airflow.decorators import dag, task
from airflow.models import Variable
from datetime import datetime, timedelta
import boto3

default_args = {
    "owner": "dataeng",
    "retries": 1,
    "retry_delay": timedelta(minutes=5)
}

@dag(
    dag_id="etl_taskflow_s3_glue_redshift",
    description="ETL: S3 -> Glue -> Redshift using TaskFlow API",
    start_date=datetime(2025, 10, 1),
    schedule_interval="@daily",
    catchup=False,
    default_args=default_args,
    tags=["aws", "etl", "glue", "redshift"]
)
def etl_taskflow():
    @task()
    def extract_from_s3(**context):
        import pandas as pd
        from io import StringIO

        s3 = boto3.client("s3")
        bucket = Variable.get("s3_raw_bucket")
        key = f"raw/sales_{context['ds']}.csv"

        print(f"Downloading s3://{bucket}/{key}")
        obj = s3.get_object(Bucket=bucket, Key=key)
        df = pd.read_csv(obj["Body"])
        print(f"Extracted {len(df)} rows")

        # Push metadata
        return {"bucket": bucket, "key": key, "row_count": len(df)}

    @task()
    def transform_with_glue(extract_info: dict):
        glue = boto3.client("glue")
        job_name = Variable.get("glue_job_name")

        args = {
            "--s3_input": f"s3://{extract_info['bucket']}/{extract_info['key']}",
            "--s3_output": f"s3://{Variable.get('s3_processed_bucket')}/processed/{{ ds }}/"
        }

        response = glue.start_job_run(JobName=job_name, Arguments=args)
        job_run_id = response["JobRunId"]
        print(f" Glue job {job_name} started with run ID {job_run_id}")

        return {"job_name": job_name, "job_run_id": job_run_id}

    @task()
    def load_to_redshift(transform_info: dict):
        redshift = boto3.client("redshift-data")

        output_path = f"s3://{Variable.get('s3_processed_bucket')}/processed/{{ ds }}/"
        table_name = Variable.get("redshift_table")

        sql = f"""
        COPY {table_name}
        FROM '{output_path}'
        IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftCopyRole'
        FORMAT AS PARQUET;
        """

        response = redshift.execute_statement(
            ClusterIdentifier="my-redshift-cluster",
            Database="analytics",
            DbUser="admin",
            Sql=sql
        )

        print(f"✅ Data loaded into Redshift table {table_name}")
        return response

    extract_info = extract_from_s3()
    transform_info = transform_with_glue(extract_info)
    load_to_redshift(transform_info)

etl_pipeline = etl_taskflow()

```

## How
| Step | Task                      | Description                                                         |
| ---- | ------------------------- | ------------------------------------------------------------------- |
| 1️⃣  | **extract_from_s3()**     | Reads the raw CSV file from S3 using `boto3`                        |
| 2️⃣  | **transform_with_glue()** | Triggers a Glue job that performs cleaning/transformation           |
| 3️⃣  | **load_to_redshift()**    | Copies processed Parquet files into Redshift using SQL COPY         |
| 🔁   | **Data Flow**             | Each `@task()` passes results via XCom automatically (TaskFlow API) |

## Airflow Connection & Variable setup
In UI: Connections
## aws_default
Conn Type: Amazon Web Services
Login/Password: Leave empty if using IAM role on EC2/ECS
## redshift_conn
Conn Type: Postgres
Host: <cluster>.redshift.amazonaws.com
Schema: analytics
Login: admin
Password: yourpasswor
Port: 5439

## In UI: Variables
| Key                   | Value                     |
| --------------------- | ------------------------- |
| `s3_raw_bucket`       | `my-raw-bucket`           |
| `s3_processed_bucket` | `my-processed-bucket`     |
| `glue_job_name`       | `etl-transform-job`       |
| `redshift_table`      | `analytics.sales_cleaned` |

## Unit Testing the DAG with Pytest
Create tests/test_dag_etl_taskflow.py

```python
from airflow.models import DagBag

def test_dag_integrity():
    """Check DAG loads without syntax or import errors."""
    dagbag = DagBag(dag_folder="dags", include_examples=False)
    assert dagbag.import_errors == {}, f"DAG import errors: {dagbag.import_errors}"

def test_task_count():
    dagbag = DagBag(dag_folder="dags", include_examples=False)
    dag = dagbag.get_dag(dag_id="etl_taskflow_s3_glue_redshift")
    assert len(dag.tasks) == 3
    task_ids = [t.task_id for t in dag.tasks]
    assert "extract_from_s3" in task_ids
    assert "transform_with_glue" in task_ids
    assert "load_to_redshift" in task_ids

```
## Run with:
pytest -v tests/



