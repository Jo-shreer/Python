## What is Airflow
Airflow is a workflow orchestration engine that runs DAGs (Directed Acyclic Graphs).
DAG = the definition of tasks and dependencies.
Tasks are instances of Operators (BashOperator, PythonOperator, SparkSubmitOperator, etc.).
Scheduler decides when to run DAG runs; Executor runs tasks.
UI shows DAG runs, task logs, retries, SLA, etc.

## 2 ‚Äî Install & run - Option A ‚Äî Quick local install with pip (Airflow 2.x)
```bash
# create venv (recommended)
python -m venv .venv && source .venv/bin/activate

# pick a constraint file matching your python version from airflow.apache.org,
# but for quick demo you can:
pip install "apache-airflow==2.6.3" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.6.3/constraints-3.10.txt"

# initialize DB
airflow db init

# create user
airflow users create \
  --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com

# start webserver (background or new terminal)
airflow webserver --port 8080

# start scheduler (in separate terminal)
airflow scheduler

# Open UI at http://localhost:8080.
```
## Option B ‚Äî Docker Compose (recommended for parity)
## Use the official Airflow image and docker-compose.yaml. Minimal example:
```yaml
# docker-compose-airflow.yml (very small example)

version: "3.8"
services:
  airflow-postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD","pg_isready","-U","airflow"]
  airflow-webserver:
    image: apache/airflow:2.6.3
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    depends_on:
      airflow-postgres:
        condition: service_healthy

# Then docker-compose -f docker-compose-airflow.yml up and open http://localhost:8080.
```
## 3 ‚Äî Anatomy of a DAG (basic)
```python

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator

default_args = {
    "owner": "you",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="simple_bash_dag",
    default_args=default_args,
    description="Simple DAG example",
    schedule_interval="@daily",
    start_date=datetime(2025, 10, 1),
    catchup=False,
) as dag:

    t1 = BashOperator(task_id="print_date", bash_command="date")
    t2 = BashOperator(task_id="sleep", bash_command="sleep 5")
    t3 = BashOperator(task_id="echo", bash_command='echo "Done"')

    t1 >> t2 >> t3

# Place file in $AIRFLOW_HOME/dags (or ./dags when using Docker Compose bind mount).
# In UI you‚Äôll see simple_bash_dag. Trigger it manually or wait for scheduler.
```
## TaskFlow API (recommended for new code)
```python

from airflow.decorators import dag, task

@dag(start_date=datetime(2025,10,1), schedule_interval=None, catchup=False)
def my_etl():
    @task
    def extract():
        return [1,2,3]

    @task
    def transform(numbers):
        return [n*2 for n in numbers]

    @task
    def load(result):
        print(result)

    load(transform(extract()))

dag = my_etl()

```
## SparkSubmitOperator (run Spark)
```python

from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

spark_task = SparkSubmitOperator(
    task_id="spark_job",
    application="/opt/airflow/dags/scripts/my_spark_job.py",
    conn_id="spark_default",
    application_args=["--input","s3://bucket/raw/","--output","s3://bucket/processed/"]
)

#Example: Corresponding Spark Script
#Let‚Äôs look at what /opt/airflow/dags/scripts/my_spark_job.py might look like

from pyspark.sql import SparkSession
import argparse

# --- 1Ô∏è‚É£ Parse CLI arguments ---
parser = argparse.ArgumentParser()
parser.add_argument("--input", required=True)
parser.add_argument("--output", required=True)
args = parser.parse_args()

# --- 2Ô∏è‚É£ Create Spark session ---
spark = SparkSession.builder.appName("AirflowETLJob").getOrCreate()

# --- 3Ô∏è‚É£ Read data from S3 ---
df = spark.read.option("header", True).csv(args.input)

# --- 4Ô∏è‚É£ Transform data ---
df_transformed = df.filter(df["status"] == "ACTIVE")

# --- 5Ô∏è‚É£ Write back to S3 ---
df_transformed.write.mode("overwrite").parquet(args.output)

spark.stop()

#So when Airflow runs the DAG, this task executes roughly this command:

 spark-submit \
  --master yarn \
  /opt/airflow/dags/scripts/my_spark_job.py \
  --input s3://bucket/raw/ \
  --output s3://bucket/processed/

# Real-World Example in a DAG
from datetime import datetime
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

with DAG(
    dag_id="spark_etl_dag",
    start_date=datetime(2025, 10, 1),
    schedule_interval="@daily",
    catchup=False,
) as dag:

    transform_data = SparkSubmitOperator(
        task_id="transform_sales_data",
        application="/opt/airflow/dags/scripts/sales_transform.py",
        conn_id="spark_default",
        application_args=[
            "--input", "s3://my-bucket/raw/sales/{{ ds }}/",
            "--output", "s3://my-bucket/processed/sales/{{ ds }}/"
        ],
    )

transform_data
#This will:
#Wait for the DAG‚Äôs schedule (daily)
#Run your Spark script on the cluster defined in spark_default
#Pass input and output paths dynamically (using templating {{ ds }} = date string)

```
## PostgresOperator / Redshift (run SQL)
```python

from airflow.providers.postgres.operators.postgres import PostgresOperator

load_task = PostgresOperator(
    task_id="create_table",
    postgres_conn_id="redshift_conn",  # connection configured in Airflow UI
    sql="""
    CREATE TABLE IF NOT EXISTS analytics.sales_summary (
      region varchar(50),
      total_sales numeric
    );
    """,
)

```
## AWS Glue operator (trigger Glue job)
```python
from airflow.providers.amazon.aws.operators.glue import AwsGlueJobOperator

glue_task = AwsGlueJobOperator(
    task_id="start_glue_job",
    job_name="my-glue-job",
    script_args={"--s3_input": "s3://my-bucket/raw/"},
    aws_conn_id="aws_default",
)


```
## 5 ‚Äî Sensors, XCom, Branching - Sensor example (wait for S3 file)
```python
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor

wait_for_file = S3KeySensor(
    task_id="wait_for_file",
    bucket_key="raw/{{ ds }}/*.csv",
    bucket_name="my-bucket",
    aws_conn_id="aws_default",
    poke_interval=60,  # seconds
    timeout=60*30,
)

```
## XCom (pass data between tasks)
```python

def push(ti):
    ti.xcom_push("my_key", {"count": 100})

def pull(ti):
    val = ti.xcom_pull(key="my_key", task_ids="push_task")
    print(val)

push_task = PythonOperator(task_id="push_task", python_callable=push)
pull_task = PythonOperator(task_id="pull_task", python_callable=pull)
push_task >> pull_task

```
## Branching (BranchPythonOperator)
```python
from airflow.operators.python import BranchPythonOperator

def choose(**kwargs):
    return "task_a" if condition else "task_b"

branch = BranchPythonOperator(
    task_id="branching",
    python_callable=choose,
)

```
## 6 ‚Äî Connections, Variables, Secrets, Templating
## Connections (UI): Admin ‚Üí Connections
Define aws_default, spark_default, redshift_conn, etc.
A connection stores host/port/user/pass and extra JSON (like role ARN).

## Variables (UI): Admin ‚Üí Variables
Small key/value store for environment-like settings. Use Variable.get("key", default_var=...).

## Secrets Backends (recommended)
Use HashiCorp Vault, AWS Secrets Manager, or environment variables rather than hardcoding credentials.
Configure AIRFLOW__SECRETS__BACKENDS in airflow.cfg or env vars.

## Jinja templating in Operators
Operators allow templated params using {{ ds }}, {{ ts }} etc.
```python

BashOperator(task_id="echo", bash_command="echo {{ ds }}")

```
## 7 ‚Äî Scheduling, catchup, retries, SLA
schedule_interval can be @daily, cron, or None (manual).
start_date is important: Airflow schedules DAG runs for the interval end.
catchup=True will run backfill jobs for missed intervals ‚Äî default can be surprising; set catchup=False if you don't want that.

Retries:
```python

default_args = {"retries": 3, "retry_delay": timedelta(minutes=5)}
from airflow.utils.task_group import TaskGroup
task.sla = timedelta(hours=1)  # if a task run exceeds SLA, an SLA miss is recorded

```
SLA:
```python

from airflow.utils.task_group import TaskGroup
task.sla = timedelta(hours=1)  # if a task run exceeds SLA, an SLA miss is recorded

```
## 8 ‚Äî Testing & Debugging
Test a DAG locally (no scheduler)
```bash
# run a single task (simulate)
airflow tasks test simple_bash_dag print_date 2025-10-14

```
## What This Command Does
This command tests a single task (not the whole DAG) in isolation, as if it were run on a specific execution date.
It does not trigger the whole DAG, nor does it affect the Airflow scheduler, database, or other tasks.
It‚Äôs used to debug or test one task manually ‚Äî like a ‚Äúdry run‚Äù.

# Command Structure
| **Part**             | **Meaning**                                       |
| -------------------- | ------------------------------------------------- |
| `airflow tasks test` | Airflow CLI command to run one task for debugging |
| `simple_bash_dag`    | The **DAG ID** where the task lives               |
| `print_date`         | The **task_id** you want to test                  |
| `2025-10-14`         | The **execution date** (logical date) to simulate |

# example from airflow import DAG
```python
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    dag_id='simple_bash_dag',
    start_date=datetime(2025, 10, 1),
    schedule_interval='@daily',
    catchup=False,
) as dag:

    print_date = BashOperator(
        task_id='print_date',
        bash_command='date'
    )

    echo_hello = BashOperator(
        task_id='echo_hello',
        bash_command='echo "Hello from Airflow!"'
    )

    print_date >> echo_hello

#In this DAG, there are two tasks:
#print_date ‚Üí prints the current date
#echo_hello ‚Üí prints a message

airflow tasks test simple_bash_dag print_date 2025-10-14

Airflow will:
Load the DAG definition (simple_bash_dag) from your dags/ folder
Find the task named print_date
Set the logical execution date to 2025-10-14
Run that task immediately (bypassing scheduler or dependencies)
Print logs and results to the console
Not mark it as success/failure in the Airflow database ‚Äî it‚Äôs just for local testing

op
[2025-10-14 00:00:00,000] {taskinstance.py:1120} INFO - Running task: print_date
[2025-10-14 00:00:00,001] {bash_operator.py:158} INFO - Running command: date
Tue Oct 14 00:00:00 UTC 2025
[2025-10-14 00:00:00,002] {taskinstance.py:1254} INFO - Task exited with return code 0


```

## Compare with Other Commands
| **Command**                                    | **Purpose**                                                    |
| ---------------------------------------------- | -------------------------------------------------------------- |
| `airflow dags trigger <dag_id>`                | Triggers a full DAG run (includes all tasks)                   |
| `airflow tasks test <dag_id> <task_id> <date>` | Runs a single task manually                                    |
| `airflow tasks run <dag_id> <task_id> <date>`  | Runs a task and records it in Airflow metadata (unlike `test`) |


## Key Difference: test vs run
| **Command**          | **Writes to DB?** | **Triggers Dependencies?** | **Use Case**              |
| -------------------- | ----------------- | -------------------------- | ------------------------- |
| `airflow tasks test` | ‚ùå No              | ‚ùå No                       | For debugging a task      |
| `airflow tasks run`  | ‚úÖ Yes             | ‚úÖ Yes                      | For forcing an actual run |

## Unit tests with pytest
```python
# tests/test_dag.py

from airflow.models.dagbag import DagBag

def test_dag_import():
    dagbag = DagBag(dag_folder="dags", include_examples=False)
    assert dagbag.import_errors == {}
    assert "simple_bash_dag" in dagbag.dags

```
## 9 ‚Äî Monitoring & Alerts
Configure email alerts in airflow.cfg or per-task email_on_failure=True.
Use external monitoring: Prometheus + Grafana exporters for Airflow metrics.
Use SLA callbacks or sensors to enforce SLAs.

## 10 ‚Äî Best Practices
Keep DAGs idempotent (re-running should be safe).
Keep tasks small and single-purpose (micro-tasks).
Use TaskGroup to group related tasks.
Avoid heavy compute in PythonOperator ‚Äî use external services (Spark, EMR, ECS)
Use params and Variables instead of hardcoding configuration.
Keep secrets out of code (use Secrets Backend).
Use XCom sparingly for small messages ‚Äî not for big payloads (use S3 instead).
Pin provider versions and use constraints file for reproducible installs.

## 11 ‚Äî Executors & Deployment options
LocalExecutor: good for dev, single machine parallelism.
CeleryExecutor: scaling with worker processes (requires broker like Redis/RabbitMQ).
KubernetesExecutor: each task runs in its own pod (great for cloud-native).
KubernetesPodOperator: run tasks as pods from within DAGs (for containerized workloads).

## Example: Small ETL DAG (S3 ‚Üí Spark ‚Üí Redshift)
```python

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator

default_args = {
    "owner": "dataeng",
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="s3_spark_redshift",
    default_args=default_args,
    schedule_interval="@daily",
    start_date=datetime(2025,10,1),
    catchup=False,
) as dag:

    wait = S3KeySensor(
        task_id="wait_for_raw",
        bucket_key="raw/{{ ds }}/*.csv",
        bucket_name="my-bucket",
        aws_conn_id="aws_default",
        poke_interval=60,
        timeout=60*30,
    )

    spark = SparkSubmitOperator(
        task_id="transform_with_spark",
        application="/opt/airflow/dags/scripts/transform.py",
        conn_id="spark_default",
        application_args=["--input", "s3://my-bucket/raw/{{ ds }}/", "--output","s3://my-bucket/processed/{{ ds }}/"],
    )

    load = PostgresOperator(
        task_id="load_to_redshift",
        postgres_conn_id="redshift_conn",
        sql="COPY analytics.table FROM 's3://my-bucket/processed/{{ ds }}/' iam_role 'arn:aws:iam::123:role/RedshiftCopyRole' FORMAT AS PARQUET;",
    )

    wait >> spark >> load

```
## 15 ‚Äî Handy Cheatsheet (quick references)
Initialize DB: airflow db init
Create user: airflow users create ...
Start webserver: airflow webserver --port 8080
Start scheduler: airflow scheduler
Test task: airflow tasks test <dag_id> <task_id> <execution_date>
List DAGs: airflow dags list
Pause DAG: airflow dags pause <dag_id>
Unpause DAG: airflow dags unpause <dag_id>

## ETL DAG that uses TaskFlow API + S3 + Glue + Redshift and show how to configure Connections/Variables
# Full ETL Pipeline DAG ‚Äî TaskFlow API + S3 + Glue + Redshift (Airflow 2.x)

This DAG performs an end-to-end ETL flow:
1Ô∏è‚É£ **Extract** ‚Üí Read data from an S3 bucket  
2Ô∏è‚É£ **Transform** ‚Üí Trigger an AWS Glue job for cleaning/enrichment  
3Ô∏è‚É£ **Load** ‚Üí Copy the transformed data into Redshift  
4Ô∏è‚É£ Uses Airflow **TaskFlow API**, AWS Connections, and Variables  

---

## Prerequisites

### 1. Airflow Providers
Install the necessary providers:
```bash
pip install apache-airflow-providers-amazon apache-airflow-providers-postgres
```
## 2. Airflow Connections (set in UI ‚Üí Admin ‚Üí Connections)
| Conn ID         | Type                | Description                                              |
| --------------- | ------------------- | -------------------------------------------------------- |
| `aws_default`   | Amazon Web Services | Use IAM Role / Access Keys                               |
| `redshift_conn` | Postgres            | Host, Port (5439), User, Password, Database for Redshift |

## 3. Airflow Variables (UI ‚Üí Admin ‚Üí Variables)
| Key                   | Example Value             | Description                 |
| --------------------- | ------------------------- | --------------------------- |
| `s3_raw_bucket`       | `my-raw-bucket`           | Bucket for raw input        |
| `s3_processed_bucket` | `my-processed-bucket`     | Bucket for transformed data |
| `glue_job_name`       | `etl-transform-job`       | Glue job to trigger         |
| `redshift_table`      | `analytics.sales_cleaned` | Target Redshift table       |

## DAG: dags/etl_taskflow_s3_glue_redshift.py
```python

from airflow.decorators import dag, task
from airflow.models import Variable
from datetime import datetime, timedelta
import boto3

default_args = {
    "owner": "dataeng",
    "retries": 1,
    "retry_delay": timedelta(minutes=5)
}

@dag(
    dag_id="etl_taskflow_s3_glue_redshift",
    description="ETL: S3 -> Glue -> Redshift using TaskFlow API",
    start_date=datetime(2025, 10, 1),
    schedule_interval="@daily",
    catchup=False,
    default_args=default_args,
    tags=["aws", "etl", "glue", "redshift"]
)
def etl_taskflow():
    @task()
    def extract_from_s3(**context):
        import pandas as pd
        from io import StringIO

        s3 = boto3.client("s3")
        bucket = Variable.get("s3_raw_bucket")
        key = f"raw/sales_{context['ds']}.csv"

        print(f"Downloading s3://{bucket}/{key}")
        obj = s3.get_object(Bucket=bucket, Key=key)
        df = pd.read_csv(obj["Body"])
        print(f"Extracted {len(df)} rows")

        # Push metadata
        return {"bucket": bucket, "key": key, "row_count": len(df)}

    @task()
    def transform_with_glue(extract_info: dict):
        glue = boto3.client("glue")
        job_name = Variable.get("glue_job_name")

        args = {
            "--s3_input": f"s3://{extract_info['bucket']}/{extract_info['key']}",
            "--s3_output": f"s3://{Variable.get('s3_processed_bucket')}/processed/{{ ds }}/"
        }

        response = glue.start_job_run(JobName=job_name, Arguments=args)
        job_run_id = response["JobRunId"]
        print(f" Glue job {job_name} started with run ID {job_run_id}")

        return {"job_name": job_name, "job_run_id": job_run_id}

    @task()
    def load_to_redshift(transform_info: dict):
        redshift = boto3.client("redshift-data")

        output_path = f"s3://{Variable.get('s3_processed_bucket')}/processed/{{ ds }}/"
        table_name = Variable.get("redshift_table")

        sql = f"""
        COPY {table_name}
        FROM '{output_path}'
        IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftCopyRole'
        FORMAT AS PARQUET;
        """

        response = redshift.execute_statement(
            ClusterIdentifier="my-redshift-cluster",
            Database="analytics",
            DbUser="admin",
            Sql=sql
        )

        print(f"‚úÖ Data loaded into Redshift table {table_name}")
        return response

    extract_info = extract_from_s3()
    transform_info = transform_with_glue(extract_info)
    load_to_redshift(transform_info)

etl_pipeline = etl_taskflow()

```

## How
| Step | Task                      | Description                                                         |
| ---- | ------------------------- | ------------------------------------------------------------------- |
| 1Ô∏è‚É£  | **extract_from_s3()**     | Reads the raw CSV file from S3 using `boto3`                        |
| 2Ô∏è‚É£  | **transform_with_glue()** | Triggers a Glue job that performs cleaning/transformation           |
| 3Ô∏è‚É£  | **load_to_redshift()**    | Copies processed Parquet files into Redshift using SQL COPY         |
| üîÅ   | **Data Flow**             | Each `@task()` passes results via XCom automatically (TaskFlow API) |

## Airflow Connection & Variable setup
In UI: Connections
## aws_default
Conn Type: Amazon Web Services
Login/Password: Leave empty if using IAM role on EC2/ECS
## redshift_conn
Conn Type: Postgres
Host: <cluster>.redshift.amazonaws.com
Schema: analytics
Login: admin
Password: yourpasswor
Port: 5439

## In UI: Variables
| Key                   | Value                     |
| --------------------- | ------------------------- |
| `s3_raw_bucket`       | `my-raw-bucket`           |
| `s3_processed_bucket` | `my-processed-bucket`     |
| `glue_job_name`       | `etl-transform-job`       |
| `redshift_table`      | `analytics.sales_cleaned` |

## Unit Testing the DAG with Pytest
Create tests/test_dag_etl_taskflow.py

```python
from airflow.models import DagBag

def test_dag_integrity():
    """Check DAG loads without syntax or import errors."""
    dagbag = DagBag(dag_folder="dags", include_examples=False)
    assert dagbag.import_errors == {}, f"DAG import errors: {dagbag.import_errors}"

def test_task_count():
    dagbag = DagBag(dag_folder="dags", include_examples=False)
    dag = dagbag.get_dag(dag_id="etl_taskflow_s3_glue_redshift")
    assert len(dag.tasks) == 3
    task_ids = [t.task_id for t in dag.tasks]
    assert "extract_from_s3" in task_ids
    assert "transform_with_glue" in task_ids
    assert "load_to_redshift" in task_ids

```
## Run with:
pytest -v tests/



