## What is Airflow
Airflow is a workflow orchestration engine that runs DAGs (Directed Acyclic Graphs).
DAG = the definition of tasks and dependencies.
Tasks are instances of Operators (BashOperator, PythonOperator, SparkSubmitOperator, etc.).
Scheduler decides when to run DAG runs; Executor runs tasks.
UI shows DAG runs, task logs, retries, SLA, etc.

## 2 — Install & run - Option A — Quick local install with pip (Airflow 2.x)
```bash
# create venv (recommended)
python -m venv .venv && source .venv/bin/activate

# pick a constraint file matching your python version from airflow.apache.org,
# but for quick demo you can:
pip install "apache-airflow==2.6.3" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.6.3/constraints-3.10.txt"

# initialize DB
airflow db init

# create user
airflow users create \
  --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com

# start webserver (background or new terminal)
airflow webserver --port 8080

# start scheduler (in separate terminal)
airflow scheduler

# Open UI at http://localhost:8080.
```
## Option B — Docker Compose (recommended for parity)
## Use the official Airflow image and docker-compose.yaml. Minimal example:
```yaml
# docker-compose-airflow.yml (very small example)
version: "3.8"
services:
  airflow-postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD","pg_isready","-U","airflow"]
  airflow-webserver:
    image: apache/airflow:2.6.3
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    depends_on:
      airflow-postgres:
        condition: service_healthy
# Then docker-compose -f docker-compose-airflow.yml up and open http://localhost:8080.
```
## 3 — Anatomy of a DAG (basic)
```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator

default_args = {
    "owner": "you",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="simple_bash_dag",
    default_args=default_args,
    description="Simple DAG example",
    schedule_interval="@daily",
    start_date=datetime(2025, 10, 1),
    catchup=False,
) as dag:

    t1 = BashOperator(task_id="print_date", bash_command="date")
    t2 = BashOperator(task_id="sleep", bash_command="sleep 5")
    t3 = BashOperator(task_id="echo", bash_command='echo "Done"')

    t1 >> t2 >> t3

# Place file in $AIRFLOW_HOME/dags (or ./dags when using Docker Compose bind mount).
# In UI you’ll see simple_bash_dag. Trigger it manually or wait for scheduler.
```
## TaskFlow API (recommended for new code)
```python
from airflow.decorators import dag, task

@dag(start_date=datetime(2025,10,1), schedule_interval=None, catchup=False)
def my_etl():
    @task
    def extract():
        return [1,2,3]

    @task
    def transform(numbers):
        return [n*2 for n in numbers]

    @task
    def load(result):
        print(result)

    load(transform(extract()))

dag = my_etl()

```
## SparkSubmitOperator (run Spark)
```python
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

spark_task = SparkSubmitOperator(
    task_id="spark_job",
    application="/opt/airflow/dags/scripts/my_spark_job.py",
    conn_id="spark_default",
    application_args=["--input","s3://bucket/raw/","--output","s3://bucket/processed/"]
)

#Example: Corresponding Spark Script
#Let’s look at what /opt/airflow/dags/scripts/my_spark_job.py might look like

from pyspark.sql import SparkSession
import argparse

# --- 1️⃣ Parse CLI arguments ---
parser = argparse.ArgumentParser()
parser.add_argument("--input", required=True)
parser.add_argument("--output", required=True)
args = parser.parse_args()

# --- 2️⃣ Create Spark session ---
spark = SparkSession.builder.appName("AirflowETLJob").getOrCreate()

# --- 3️⃣ Read data from S3 ---
df = spark.read.option("header", True).csv(args.input)

# --- 4️⃣ Transform data ---
df_transformed = df.filter(df["status"] == "ACTIVE")

# --- 5️⃣ Write back to S3 ---
df_transformed.write.mode("overwrite").parquet(args.output)

spark.stop()

#So when Airflow runs the DAG, this task executes roughly this command:

 spark-submit \
  --master yarn \
  /opt/airflow/dags/scripts/my_spark_job.py \
  --input s3://bucket/raw/ \
  --output s3://bucket/processed/

# Real-World Example in a DAG
from datetime import datetime
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

with DAG(
    dag_id="spark_etl_dag",
    start_date=datetime(2025, 10, 1),
    schedule_interval="@daily",
    catchup=False,
) as dag:

    transform_data = SparkSubmitOperator(
        task_id="transform_sales_data",
        application="/opt/airflow/dags/scripts/sales_transform.py",
        conn_id="spark_default",
        application_args=[
            "--input", "s3://my-bucket/raw/sales/{{ ds }}/",
            "--output", "s3://my-bucket/processed/sales/{{ ds }}/"
        ],
    )

transform_data
#This will:
#Wait for the DAG’s schedule (daily)
#Run your Spark script on the cluster defined in spark_default
#Pass input and output paths dynamically (using templating {{ ds }} = date string)

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```
## 4 — Key Operators & examples - PythonOperator (call a Python function)
```python

```

```






