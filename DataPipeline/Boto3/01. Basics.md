# ‚òÅÔ∏è Boto3 ‚Äî AWS SDK for Python

## üí° What is Boto3?

**Boto3** is the **official AWS SDK (Software Development Kit) for Python**.

It allows you to **interact with AWS services programmatically**, such as:
- **S3** (Simple Storage Service)
- **EC2** (Elastic Compute Cloud)
- **Lambda**
- **DynamoDB**
- **Glue**
- **Redshift**
- and many more...

---

## ‚öôÔ∏è Why Use Boto3?

‚úÖ Automate AWS tasks (no manual console work)  
‚úÖ Build ETL pipelines (S3 ‚Üí Glue ‚Üí Redshift)  
‚úÖ Manage infrastructure (create/delete EC2, buckets, etc.)  
‚úÖ Integrate AWS services into Python apps or Airflow DAGs  

## üß± Installing Boto3

```bash
pip install boto3
```

# üîë Configuring AWS Credentials
Boto3 needs your AWS Access Key and Secret Key to authenticate.
You can configure credentials in several ways:
```bash
aws configure
~/.aws/credentials
~/.aws/config

```
# Directly in Code (not recommended for production)
```python
import boto3

session = boto3.Session(
    aws_access_key_id="YOUR_ACCESS_KEY",
    aws_secret_access_key="YOUR_SECRET_KEY",
    region_name="us-east-1"
)
```
# Create S3 Client

```python
import boto3
s3 = boto3.client('s3')
```
# List Buckets
```python
response = s3.list_buckets()
for bucket in response['Buckets']:
    print(bucket['Name'])

```
#  Upload File to S3
```python
import boto3
s3 = boto3.client('s3')
```
#  Upload File to S3 Client
```python
s3.upload_file("local_file.csv", "my-bucket", "data/local_file.csv")
print("‚úÖ File uploaded to S3")

```
#  Download File from S3
```python
s3.download_file("my-bucket", "data/local_file.csv", "downloaded.csv")

```
#  List Objects in a Bucket 
```python
objects = s3.list_objects_v2(Bucket="my-bucket", Prefix="data/")
for obj in objects.get('Contents', []):
    print(obj['Key'])

```
#  Delete an Object 
```python
s3.delete_object(Bucket="my-bucket", Key="data/local_file.csv")
print("üóëÔ∏è File deleted")

```
#  Using S3 Resource (Object-oriented) 
```python
s3_resource = boto3.resource('s3')
bucket = s3_resource.Bucket('my-bucket')

for obj in bucket.objects.all():
    print(obj.key)

```
#  Example: Upload Pandas DataFrame to S3 as CSV 
```python
import pandas as pd
import boto3
from io import StringIO

# Create DataFrame
df = pd.DataFrame({'name': ['Alice', 'Bob'], 'score': [90, 85]})

# Convert to CSV buffer
csv_buffer = StringIO()
df.to_csv(csv_buffer, index=False)

# Upload to S3
s3 = boto3.client('s3')
s3.put_object(Bucket='my-bucket', Key='reports/scores.csv', Body=csv_buffer.getvalue())

print("‚úÖ DataFrame uploaded to S3 as CSV")

``` 
# üîë AWS Glue Example 
```python
glue = boto3.client('glue')

# Start a Glue job
response = glue.start_job_run(JobName='my-glue-job')
print("üöÄ Glue job started:", response['JobRunId'])

```
#  Redshift Example (copy from S3) 
```python
redshift = boto3.client('redshift-data')

query = """
COPY my_schema.sales
FROM 's3://my-bucket/data/sales.csv'
IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftCopyRole'
FORMAT AS CSV
IGNOREHEADER 1;
"""

response = redshift.execute_statement(
    ClusterIdentifier='my-redshift-cluster',
    Database='analytics',
    DbUser='admin',
    Sql=query
)
print("‚úÖ Data copied to Redshift")

```
#  DynamoDB Example 
```python
dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('Users')

# Insert item
table.put_item(Item={'UserId': '123', 'Name': 'Alice', 'Age': 25})

# Get item
response = table.get_item(Key={'UserId': '123'})
print(response['Item'])

```
## Real-World Example: ETL Flow (S3 ‚Üí Glue ‚Üí Redshift)
```python
import boto3

# 1Ô∏è‚É£ Get data from S3
s3 = boto3.client('s3')
s3.download_file('my-bucket', 'raw/data.csv', '/tmp/data.csv')

# 2Ô∏è‚É£ Trigger Glue job for transformation
glue = boto3.client('glue')
response = glue.start_job_run(JobName='transform-data-job')
print("üöÄ Glue job started:", response['JobRunId'])

# 3Ô∏è‚É£ Load to Redshift
redshift = boto3.client('redshift-data')
query = """
COPY analytics.cleaned_data
FROM 's3://my-bucket/processed/data/'
IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftCopyRole'
FORMAT AS PARQUET;
"""
redshift.execute_statement(
    ClusterIdentifier='my-cluster',
    Database='analytics',
    DbUser='admin',
    Sql=query
)
print("‚úÖ Data successfully loaded into Redshift")

```
## Best Practice
| ‚úÖ Do                                                | ‚ùå Don‚Äôt                          |
| --------------------------------------------------- | -------------------------------- |
| Use **IAM roles** instead of hardcoding credentials | Store credentials in code        |
| Use **pagination** when listing many S3 objects     | Assume all objects fit in memory |
| Reuse clients (`boto3.client()`)                    | Recreate clients for each call   |
| Use **try/except** for error handling               | Assume all API calls succeed     |
| Use **async or multithreading** for large transfers | Run one API call at a time       |



